<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		 
			
  
    <meta name="twitter:card" content="summary"/>
    
      <meta name="twitter:image" content="https://ealebed.github.io/images/avatar.png" />
    
  
  
  <meta name="twitter:title" content="Установка и настройка GlusterFS-server 3.5.2 в ОС Debian"/>
  <meta name="twitter:description" content="GlusterFS — простая в настройке и использовании распределенная файловая система, работающая поверх основной файловой системы. Давайте разберемся с установкой и GlusterFS-server в ОС Debian Wheezy!"/>
  
    <meta name="twitter:site" content="@ealebed"/>
  
  
  
  
    <meta name="twitter:creator" content="@Yevhen Lebid"/>
  



		
		<meta name="author" content="Yevhen Lebid">
		<meta name="description" content="Yevhen Lebid&#39;s website">
		<meta name="generator" content="Hugo 0.104.3" />
		
  			
			<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112453311-1"></script>
			<script>
			  window.dataLayer = window.dataLayer || [];
			  function gtag(){dataLayer.push(arguments);}
			  gtag('js', new Date());

			  gtag('config', 'UA-112453311-1');
			</script>
		
		<title>Установка и настройка GlusterFS-server 3.5.2 в ОС Debian &middot; Yevhen Lebid&#39;s website</title>
		<link rel="shortcut icon" href="https://ealebed.github.io/images/favicon.ico">
		<link rel="stylesheet" href="https://ealebed.github.io/css/style.css">
		<link rel="stylesheet" href="https://ealebed.github.io/css/highlight.css">

		
		<link rel="stylesheet" href="https://ealebed.github.io/css/font-awesome.min.css">
		

		
		<link href="https://ealebed.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Yevhen Lebid&#39;s website" />
		


		
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='https://ealebed.github.io/'> <span class="arrow">←</span>Home</a>
	
	<a href='https://ealebed.github.io/posts'>Archive</a>
	<a href='https://ealebed.github.io/tags'>Tags</a>
	<a href='https://ealebed.github.io/about'>About</a>

	

	
		<a class="cta" href="https://ealebed.github.io/index.xml">RSS</a>
	
</nav>


        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        Установка и настройка GlusterFS-server 3.5.2 в ОС Debian
                    </h1>
                    <h2 class="headline">
                    Oct 24, 2016 22:41
                    · 684 words
                    · 4 minute read
                      <span class="tags">
                      
                      
                          
                              <a href="https://ealebed.github.io/tags/glusterfs">glusterfs</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                <section id="post-body">
                    <p><strong>GlusterFS</strong> — простая в настройке и использовании распределенная файловая система, работающая поверх основной файловой системы. Давайте разберемся с <strong>установкой и GlusterFS-server в ОС Debian Wheezy</strong>!</p>
<p>Прежде всего, хочу объяснить, почему был выбран <strong>GlusterFS-server</strong> версии 3.5.2 — далеко не самой свежей на момент написания статьи. В моем случае, на нескольких серверах под управлением ОС Debian Wheezy уже был настроен и успешно использовался <strong>GlusterFS-server</strong> версии 3.2.7, но при необходимости добавления новых узлов в кластер возникли сложности. Как выяснилось, для расширения кластера нужен <strong>GlusterFS-server</strong> версии не ниже 3.3, а в репозитории wheezy-backports как раз нашлась версия 3.5.2.</p>
<p>Считаем, что все хосты будущего кластера доступны друг для друга, в firewall открыты TCP порты 111, 24007-24050 и созданы все необходимы каталоги/точки монтирования.</p>
<p><strong>Примечание.</strong> В нашем примере GlusterFS настраивается как <code>replicated volume</code> на 5 машинах с локальными ip-адресами 192.168.0.1, 192.168.0.3, 192.168.0.7, 192.168.0.13 и 192.168.0.14</p>
<p><strong>Устанавливаем GlusterFS-server</strong> на всех серверах:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>aptitude install -t wheezy-backports glusterfs-server
</span></span></code></pre></div><p>Сразу после установки проверим состояние кластера:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster peer status
</span></span><span style="display:flex;"><span>Number of Peers: <span style="color:#ae81ff">0</span>
</span></span></code></pre></div><p>Подключаемся на любой хост из нашего списка (например, 192.168.0.1) и создаем кластер:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster peer probe 192.168.0.3
</span></span><span style="display:flex;"><span>peer probe: success.
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster peer probe 192.168.0.7
</span></span><span style="display:flex;"><span>peer probe: failed: 192.168.0.7 is already part of another cluster
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster peer probe 192.168.0.13
</span></span><span style="display:flex;"><span>peer probe: success.
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster peer probe 192.168.0.14
</span></span><span style="display:flex;"><span>peer probe: success.
</span></span></code></pre></div><p>Добавлять в список хостов кластера «самого себя» не нужно:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster peer probe 192.168.0.1
</span></span><span style="display:flex;"><span>peer probe: success. Probe on localhost not needed
</span></span></code></pre></div><p>Если вы ошибетесь и попробуете добавить один и тот же сервер дважды, то увидите примерно следующее:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster peer probe 192.168.0.3
</span></span><span style="display:flex;"><span>peer probe: success. Host 192.168.0.3 port <span style="color:#ae81ff">24007</span> already in peer list
</span></span></code></pre></div><p>Как вы наверняка заметили, что-то пошло не так при добавлении узла с ip-адресом 192.168.0.7. Проверяем состояние нашего кластера:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster peer status
</span></span><span style="display:flex;"><span>Number of Peers: <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Hostname: 192.168.0.7
</span></span><span style="display:flex;"><span>Uuid: 1b138a89-64dc-4a5e-8095-8892abaaf0e5
</span></span><span style="display:flex;"><span>State: Accepted peer request <span style="color:#f92672">(</span>Connected<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Hostname: 192.168.0.3
</span></span><span style="display:flex;"><span>Uuid: b9789ef4-1f26-4320-a7ba-0902c3434d76
</span></span><span style="display:flex;"><span>State: Peer in Cluster <span style="color:#f92672">(</span>Connected<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Hostname: 192.168.0.13
</span></span><span style="display:flex;"><span>Uuid: c8618f71-fdb3-4f28-8266-9eebca69e4e2
</span></span><span style="display:flex;"><span>State: Peer in Cluster <span style="color:#f92672">(</span>Connected<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Hostname: 192.168.0.14
</span></span><span style="display:flex;"><span>Uuid: 92876551-847a-4a9d-9a86-07c23b3e91f0
</span></span><span style="display:flex;"><span>State: Peer in Cluster <span style="color:#f92672">(</span>Connected<span style="color:#f92672">)</span>
</span></span></code></pre></div><p>Мы не можем создать кластер, так как один из его узлов находится в состоянии <code>Accepted peer request</code>. В моем случае всему виной старые конфиги <strong>GlusterFS-server</strong> 3.2.7, поэтому делаем так:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster peer detach 192.168.0.7
</span></span><span style="display:flex;"><span>peer detach: success
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster peer probe 192.168.0.7
</span></span><span style="display:flex;"><span>peer probe: success.
</span></span></code></pre></div><p>Теперь состояние узлов соответствует нашим ожиданиям:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster peer status
</span></span><span style="display:flex;"><span>Number of Peers: <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Hostname: 192.168.0.13
</span></span><span style="display:flex;"><span>Uuid: c8618f71-fdb3-4f28-8266-9eebca69e4e2
</span></span><span style="display:flex;"><span>State: Peer in Cluster <span style="color:#f92672">(</span>Connected<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Hostname: 192.168.0.3
</span></span><span style="display:flex;"><span>Uuid: b9789ef4-1f26-4320-a7ba-0902c3434d76
</span></span><span style="display:flex;"><span>State: Peer in Cluster <span style="color:#f92672">(</span>Connected<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Hostname: 192.168.0.14
</span></span><span style="display:flex;"><span>Uuid: 92876551-847a-4a9d-9a86-07c23b3e91f0
</span></span><span style="display:flex;"><span>State: Peer in Cluster <span style="color:#f92672">(</span>Connected<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Hostname: 192.168.0.7
</span></span><span style="display:flex;"><span>Uuid: 5837f180-6b00-403c-83ff-e993ef7bf4ef
</span></span><span style="display:flex;"><span>State: Peer in Cluster <span style="color:#f92672">(</span>Connected<span style="color:#f92672">)</span>
</span></span></code></pre></div><p>Можно сделать аналогичные проверки со всех хостов кластера. Теперь создадим реплицируемый том (<code>volume</code>):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster volume create www replica <span style="color:#ae81ff">5</span> transport tcp 192.168.0.1:/var/www/testsite 192.168.0.7:/var/www/testsite 192.168.0.3:/var/www/testsite 192.168.0.13:/var/www/testsite 192.168.0.14:/var/www/testsite
</span></span><span style="display:flex;"><span>volume create: www: success: please start the volume to access data
</span></span></code></pre></div><p>Запускаем созданный том:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster volume start www
</span></span><span style="display:flex;"><span>volume start: www: success
</span></span></code></pre></div><p>После чего проверим статус нашего тома:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster volume status
</span></span><span style="display:flex;"><span>Status of volume: www
</span></span><span style="display:flex;"><span>Gluster process						Port	Online	Pid
</span></span><span style="display:flex;"><span>------------------------------------------------------------------------------
</span></span><span style="display:flex;"><span>Brick 192.168.0.1:/var/www/testsite			49152	Y	<span style="color:#ae81ff">29660</span>
</span></span><span style="display:flex;"><span>Brick 192.168.0.7:/var/www/testsite			49152	Y	<span style="color:#ae81ff">13559</span>
</span></span><span style="display:flex;"><span>Brick 192.168.0.3:/var/www/testsite			49153	Y	<span style="color:#ae81ff">7743</span>
</span></span><span style="display:flex;"><span>Brick 192.168.0.13:/var/www/testsite			49152	Y	<span style="color:#ae81ff">28423</span>
</span></span><span style="display:flex;"><span>Brick 192.168.0.14:/var/www/testsite			49152	Y	<span style="color:#ae81ff">21147</span>
</span></span><span style="display:flex;"><span>NFS Server on localhost					2049	Y	<span style="color:#ae81ff">29674</span>
</span></span><span style="display:flex;"><span>Self-heal Daemon on localhost				N/A	Y	<span style="color:#ae81ff">29679</span>
</span></span><span style="display:flex;"><span>NFS Server on 192.168.0.14				2049	Y	<span style="color:#ae81ff">21161</span>
</span></span><span style="display:flex;"><span>Self-heal Daemon on 192.168.0.14			N/A	Y	<span style="color:#ae81ff">21167</span>
</span></span><span style="display:flex;"><span>NFS Server on 192.168.0.13				2049	Y	<span style="color:#ae81ff">28437</span>
</span></span><span style="display:flex;"><span>Self-heal Daemon on 192.168.0.13			N/A	Y	<span style="color:#ae81ff">28442</span>
</span></span><span style="display:flex;"><span>NFS Server on 192.168.0.3				2049	Y	<span style="color:#ae81ff">7761</span>
</span></span><span style="display:flex;"><span>Self-heal Daemon on 192.168.0.3				N/A	Y	<span style="color:#ae81ff">7766</span>
</span></span><span style="display:flex;"><span>NFS Server on 192.168.0.7				2049	Y	<span style="color:#ae81ff">13631</span>
</span></span><span style="display:flex;"><span>Self-heal Daemon on 192.168.0.7				N/A	Y	<span style="color:#ae81ff">13642</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Task Status of Volume www
</span></span><span style="display:flex;"><span>------------------------------------------------------------------------------
</span></span><span style="display:flex;"><span>There are no active volume tasks
</span></span></code></pre></div><p>Интересующую информацию о реплицируемом томе можно еще посмотреть так:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>gluster volume info
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Volume Name: www
</span></span><span style="display:flex;"><span>Type: Replicate
</span></span><span style="display:flex;"><span>Volume ID: 782e8fd8-0a72-4966-9590-77c244bce3b6
</span></span><span style="display:flex;"><span>Status: Started
</span></span><span style="display:flex;"><span>Number of Bricks: <span style="color:#ae81ff">1</span> x 5 <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>Transport-type: tcp
</span></span><span style="display:flex;"><span>Bricks:
</span></span><span style="display:flex;"><span>Brick1: 192.168.0.1:/var/www/testsite
</span></span><span style="display:flex;"><span>Brick2: 192.168.0.7:/var/www/testsite
</span></span><span style="display:flex;"><span>Brick3: 192.168.0.3:/var/www/testsite
</span></span><span style="display:flex;"><span>Brick4: 192.168.0.13:/var/www/testsite
</span></span><span style="display:flex;"><span>Brick5: 192.168.0.14:/var/www/testsite
</span></span></code></pre></div><p>Добавляем в <code>/etc/fstab</code> на первом узле (192.168.0.1) следующую строку:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>localhost:/www    /var/www/svnup             glusterfs rw,nosuid,nodev,exec,nouser,async,_netdev  0 0
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>Монтируем:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mount /var/www/svnup
</span></span></code></pre></div><p>Теперь можно &lsquo;заливать&rsquo; данные в каталог <code>/var/www/svnup</code> на первом узле кластера и они будут легко и просто реплицироваться на все 5 хостов к каталог <code>/var/www/testsite</code>. Это очень удобно, например, для распределения web-контента на бэкэндах.</p>
                </section>
            </article>

            
                <a class="twitter" href="https://twitter.com/intent/tweet?text=https%3a%2f%2fealebed.github.io%2fposts%2f2016%2f%25D1%2583%25D1%2581%25D1%2582%25D0%25B0%25D0%25BD%25D0%25BE%25D0%25B2%25D0%25BA%25D0%25B0-%25D0%25B8-%25D0%25BD%25D0%25B0%25D1%2581%25D1%2582%25D1%2580%25D0%25BE%25D0%25B9%25D0%25BA%25D0%25B0-glusterfs-server-3.5.2-%25D0%25B2-%25D0%25BE%25D1%2581-debian%2f - %d0%a3%d1%81%d1%82%d0%b0%d0%bd%d0%be%d0%b2%d0%ba%d0%b0%20%d0%b8%20%d0%bd%d0%b0%d1%81%d1%82%d1%80%d0%be%d0%b9%d0%ba%d0%b0%20GlusterFS-server%203.5.2%20%d0%b2%20%d0%9e%d0%a1%20Debian by @ealebed"><span class="icon-twitter"> tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>

            

            
                <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'ealebed'; 

     
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

            

            
                <ul id="post-list" class="archive readmore">
    <h3>Read more</h3>

    
    
    
        <li>
            <a href="https://ealebed.github.io/posts/2021/healthcheck-for-apache-airflow-in-k8s-cluster/">Healthcheck для Apache Airflow в Kubernetes кластере<aside class="dates">Aug 26 2021</aside></a>
        </li>
    
        <li>
            <a href="https://ealebed.github.io/posts/2021/apache-airflow-and-slack-integration/">Интеграция Apache Airflow и Slack для отправки уведомлений<aside class="dates">Jul 12 2021</aside></a>
        </li>
    
        <li>
            <a href="https://ealebed.github.io/posts/2021/apache-airflow-%D0%B7%D0%B0%D0%BF%D1%83%D1%81%D0%BA-kubernetespodoperator-%D1%87%D0%B5%D1%80%D0%B5%D0%B7-api/">Apache Airflow: запуск Kubernetes Pod Operator через API<aside class="dates">May 22 2021</aside></a>
        </li>
    
        <li>
            <a href="https://ealebed.github.io/posts/2021/%D0%B1%D0%B5%D0%B7%D0%BE%D0%BF%D0%B0%D1%81%D0%BD%D0%B0%D1%8F-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0-%D1%81-%D1%81%D0%B5%D0%BA%D1%80%D0%B5%D1%82%D0%B0%D0%BC%D0%B8-%D0%BF%D1%80%D0%B8-%D1%81%D0%B1%D0%BE%D1%80%D0%BA%D0%B5-docker-%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2/">Безопасная работа с секретами при сборке docker-образов<aside class="dates">Apr 30 2021</aside></a>
        </li>
    
        <li>
            <a href="https://ealebed.github.io/posts/2021/%D0%B2%D0%B0%D0%BB%D0%B8%D0%B4%D0%B0%D1%86%D0%B8%D1%8F-%D0%BC%D0%B8%D0%B3%D1%80%D0%B0%D1%86%D0%B8%D0%B9-flyway-%D1%81-%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D1%8C%D1%8E-testcontainers/">Валидация миграций flyway c помощью testcontainers<aside class="dates">Feb 5 2021</aside></a>
        </li>
    
</ul>

            

            <footer id="footer">
    
        <div id="social">
	
	
    <a class="symbol" href="https://www.facebook.com/ealebed">
        <i class="fa fa-facebook-square"></i>
    </a>
    
    <a class="symbol" href="https://www.github.com/ealebed">
        <i class="fa fa-github-square"></i>
    </a>
    
    <a class="symbol" href="https://www.linkedin.com/in/ealebed/">
        <i class="fa fa-linkedin-square"></i>
    </a>
    
    <a class="symbol" href="https://www.twitter.com/ealebed">
        <i class="fa fa-twitter-square"></i>
    </a>
    

</div>

    
    <p class="small">
    
       © Copyright 2023 <i class="fa fa-heart" aria-hidden="true"></i> Yevhen Lebid
    
    </p>
</footer>

        </section>

        <script src="https://ealebed.github.io/js/jquery-2.2.4.min.js"></script>
<script src="https://ealebed.github.io/js/main.js"></script>
<script src="https://ealebed.github.io/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>





    </body>
</html>
